{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a712b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "# from IPython.display import Markdown, display\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9800dbee-2f9d-4ada-80de-830d73a6077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f7baa-1c0a-430b-981b-83ddca9e71f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using GPT Tree Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "775863b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from llama_index import PromptHelper\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 1024\n",
    "# set number of output tokens\n",
    "num_output = 128\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 5  # 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    # model_name = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "    # model_name = 'EleutherAI/gpt-neo-125M'\n",
    "    model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "    # model_name = 'facebook/opt-iml-max-1.3b'\n",
    "    pipeline = pipeline(\n",
    "        'text-generation', model=model_name, device='cuda:0', model_kwargs={'torch_dtype': torch.bfloat16}\n",
    "    )\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        prompt_length = len(prompt)\n",
    "        response = self.pipeline(prompt, temperature=0.9, max_new_tokens=num_output)[0]['generated_text']\n",
    "\n",
    "        # only return newly generated tokens\n",
    "        return response[prompt_length:]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {'name_of_model': self.model_name}\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d5e38e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from llama_index import PromptHelper\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 2048\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    model_name = 'google/flan-t5-large'\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to('cuda')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # prompt_length = len(prompt)\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "        response = self.model.generate(**inputs, temperature=1, max_new_tokens=num_output)\n",
    "        response = self.tokenizer.batch_decode(response, skip_special_tokens=True)[0]\n",
    "\n",
    "        return response\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {'name_of_model': self.model_name}\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c297fd3-3424-41d8-9d0d-25fe6310ab62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor, ServiceContext\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "# service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881f151-279e-4910-95c7-f49d3d6a4c69",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0b2364-4806-4656-81e7-3f6e4b910b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('paul_graham_essay/data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd14686d-1c53-4637-9340-3745f2121ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 22214 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>I was in decent shape at painting and drawing from the RISD foundation that summer, but I still don't know how I managed to pass the written exam. I remember that I answered the essay question by writing about Cezanne, and that I cranked up the intellectual level as high as I could to make the most of my limited vocabulary. [2] I'm only up to age 25 and already there are such conspicuous patterns. Here I was, yet again about to attend some august institution in the hopes of learning about some prestigious subject, and yet again about to be disappointed.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import GPTListIndex\n",
    "\n",
    "index = GPTListIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "response = index.query(\"Answer the question: what did the author do growing up?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "370fd08f-56ff-4c24-b0c4-c93116a6d482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.indices.query.tree.leaf_query:> Starting query: What did the author do growing up?\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1595 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>writing and programming</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import GPTTreeIndex\n",
    "\n",
    "index = GPTTreeIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "response = index.query(\"What did the author do growing up?\", child_branch_factor=1)\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "# index = GPTListIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee5b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.indices.query.tree.retrieve_query:> Starting query: What did the author do growing up?\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1584 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>writing and programming</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import SummaryPrompt\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import GPTTreeIndex\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "\n",
    "query_str = \"What did the author do growing up?\"\n",
    "SUMMARY_PROMPT_TMPL = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    f\"answer the question: {query_str}\\n\"\n",
    ")\n",
    "SUMMARY_PROMPT = SummaryPrompt(SUMMARY_PROMPT_TMPL)\n",
    "index_with_query = GPTTreeIndex.from_documents(\n",
    "    documents, service_context=service_context, summary_template=SUMMARY_PROMPT\n",
    ")\n",
    "\n",
    "# directly retrieve response from root nodes instead of traversing tree\n",
    "response = index_with_query.query(query_str, mode=\"retrieve\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6457769-dfaf-4241-ab32-dcf901dde902",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using GPT Keyword Table Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d59ef6-70b0-47bb-818d-7237a3b7de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTKeywordTableIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a3f1c67-6d73-4f37-afcf-9e637002fcff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vyacharin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 18765 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# build keyword index\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d4f686-6825-49cf-a113-d2fdd484de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "Extracted keywords: ['y combinator', 'combinator']\n",
      "> Querying with idx: 7143669651211954504: of excluding them, because there were so many s...\n",
      "> Querying with idx: 4978118451876167434: browser, and then host the resulting applicatio...\n",
      "> Querying with idx: 7378313280237489139: person, and from those we picked 8 to fund. The...\n",
      "> Querying with idx: 2670584622494666310: it was like living in another country, and sinc...\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a483514d-4ab5-489d-8b99-7250df491ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "After a few years, the author decided to step away from Y Combinator to focus on other projects, such as painting and writing essays. In 2013, he handed over control of Y Combinator to Sam Altman. The author's mother passed away in 2014, and after taking some time to grieve, he returned to writing essays and working on Lisp. He continued working on Lisp until 2019, when he finally completed the project.\n",
       "\n",
       "In 2015, the author decided to move to England with his family. They originally intended to only stay for a year, but ended up liking it so much that they remained there. The author wrote Bel while living in England. In 2019, he finally finished the project. After completing Bel, the author wrote a number of essays on various topics. He continued writing essays through 2020, but also started thinking about other things he could work on.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17261694-812f-4411-ac84-6cd9a007c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import GPTSimpleVectorIndex, LangchainEmbedding\n",
    "\n",
    "# load in HF embedding model from langchain\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    prompt_helper=prompt_helper,\n",
    "    embed_model=embed_model,\n",
    "    # chunk_size_limit=chunk_size_limit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c30859b0-9632-411e-9b81-290a259e73ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c22716b0fd945039dfd88e6e8107de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dce65b9cd34d3dbfcf18ab1069c704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231051d0078a42ddaa6c3a7d3185d314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb1559cc7e14e1f8355bd71e5a6201c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec83fb3f50754da98417f920d8280555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b99d2ccad464ae88e7f4b7420ef8e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 17617 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a856ed4-d2b8-4e53-8f77-4b229767212f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3008f0ef7d944138f1702d4741acc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3875 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While I was a student at the Accademia I started painting still lives in my bedroom at night. These paintings were tiny, because the room was, and because I painted them on leftover scraps of canvas, which was all I could afford at the time. Painting still\n"
     ]
    }
   ],
   "source": [
    "# query will use the same embed_model\n",
    "response = index.query(\n",
    "    \"What did the author do growing up?\", \n",
    "    mode=\"embedding\", \n",
    "    verbose=False, \n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a701b-3920-4de3-855f-415c6b5c0ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
